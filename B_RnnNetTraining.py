# -*- coding: utf-8 -*-
__author__ = 'Hossein Mehri'
__license__ = 'MIT'
"""
Created on Fri May 21 11:04:38 2021

@author: Hossein Mehri

This code trains the RNN network using the traffic patterns generated by 
"A_GeneratingTrafficPatterns.py". The patterns are first processed as batches,
then are fed to the RNN network. Note that two types of RNN networks are used 
in this work: LSTM and GRU.

This code executes the second of six steps in predicting the mMTC network's traffic
using the Fast LiveStream Predictor (FLSP) algorithm, a new live forecasting algorithm.

 --------------------------      vvvvvvvvvvvvvvv      ------------------------------------
|GeneratingTrafficPatterns| --> |RnnNetTraining| --> |GeneratingBurstDetNetFeedDataForRNN| -->
--------------------------      ^^^^^^^^^^^^^^^      ------------------------------------
     -------------------------------      ---------------------------------------------
--> |BurstDetNetTrainingWithRNNData| --> |EvaluatingNetworksAndGeneratingResultsForRNN| -->
    -------------------------------      ---------------------------------------------
     ------------------------------------------
--> |PlottingResultsAndExportingToMatlabForRNN|
    ------------------------------------------

The code loads required data from "A_generatedTraffic" folder.
The trained network stored in "B_trainedRNN" folder.

You can modify the following parameters:
    - nType: Network type: [LSTM, GRU];
    - rnnHiddenSize: LSTM/GRU hidden layer size;
    - rnnLayers: number of LSTM/GRU layers;
    - dropoutVal: Dropout probability between DenseNet layers;
    - batchSize: Batch size of input data to ML network;
    - repeating: Number of iterations over the training dataset;
    - TpVec: A list shwoing prediction duration in the validation phase;
    - Tstep: The frequency of collecting new data from the network in live network;
    - Tseed: The initializing data length;
    - trainingDataSize: Number of patterns used for training;
    - validationDataSize: Number of patterns used for validation;
    - validationFreq: Running validation every validationFreq epochs.

"""

import torch
from torch import nn
from torch import optim
import numpy as np
from tqdm import tqdm

from datetime import datetime
from MLNetworks import rnnNetwork
from MLNetworks import dataManagement, list2Tensor, preprocess
    


def validation(model, batchSize, valInput, valTarget, seedLength, batchesAhead,\
               feedLength, criterion):
    
    """
    This function evaluates the performance of a trained ML model in online mode.
    In online mode, we get a new bunch of data, called 'feed', in 'feedLength'
    batches frequncy. Upon receiving the 'feed' data, we generate the prediction
    bunches for 'batchesAhead' batches, but only keep last 'feedLength' batches 
    which are added to the final sequence. This function measures the loss value
    between the predicted and ground truth data and returns the loss value.
    
    validationLoss = validation(model, batchSize, valInput, valTarget, seedLength,\
                                batchesAhead, feedLength, criterion):

    Parameters
    ----------
    model : rnnNetwork
        Trained ML network which is going to be evaluated.
    batchSize : int
        Batch size.
    valInput : tensor
        Input sequence in batched format.
    valTarget : tensor
        Target sequence in batched format.
    seedLength : int
        Length of seed data in batches.
    batchesAhead : int
        Prediction length at each step in batches.
    feedLength : int
        Feed length and frequency of receiveing feed data.
    criterion : func
        Torch function used to calculate the error value.

    Returns
    -------
    loss : float
        Calculated error value.

    """
    model.eval() # Evaluation mode activation.
    model.init_hidden(batchSize) # Initializing the ML network states.
    # Implementing the online mode prediction:
    numOfFeedData=int((valInput.size(0)-seedLength-batchesAhead)/feedLength) # Online steps.
    feedData=valInput[0+seedLength:(numOfFeedData*feedLength)\
                       +seedLength].view(-1,feedLength,batchSize,valInput.size(2)) # Online bunches of data.
    seed=valInput[0:seedLength,:,:] # Seed extraction from the input sequence.
    output = model(seed, steps=batchesAhead, eval = True) # Initializing the model.
    # Feeding the online-collected data, 'feed', to the ML network and generate the predictions.
    for i,feed in enumerate(feedData):
        out = model(feed, steps=batchesAhead, eval = True) # Predicted bunch of data at each step.
        output=torch.cat((output,out[-1*feedLength:]),0) # Keep only last 'feedLength' batches from each bunch.
    loss=criterion(output,valTarget[0:output.size(0)]).item()
    return loss

#%% Main Function:
if __name__=="__main__":
    # ML network settings that can be changed:
    nType='GRU' # Network type: [LSTM, GRU]
    rnnHiddenSize=2500 # LSTM hidden layer size.
    rnnLayers=2 # number of LSTM layers
    dropoutVal = 0.4 # Dropout probability between DenseNet layers.
    # Simulation settings that can be changed:
    batchSize=25 # batch size of input data to ML network.
    repeating=1  # Number of iterations over the training dataset.
    TpVec=[1,1.5,2] # Predicting Tp seconds ahead during the validation
    Tstep=0.5 # The frequency of collecting new data from the network in ...\
              # online mode prediction in seconds.
    Tseed=30 # The initializing data length in seconds.
    trainingDataSize=3 # Number of patterns used for training.
    validationDataSize=2 # Number of patterns used for validation.
    validationFreq=30    # Running validation every validationFreq epochs
    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    # Fixed parameters:
    frameSize=0.005 # Duration of each time slot.
    seedLength=int(Tseed/frameSize/batchSize) # The initializing data length in batches
    stepLength=int(Tstep/frameSize/batchSize) # The length of new online data in batches
    epochs=repeating*trainingDataSize  # Old definition of epochs
    # Load Data:
    (dataLoad, patternDataAddress)=dataManagement(path='A_generatedTraffic',\
                                                  includesStr='MTC_Traffic',\
                                                  returnAddress=True)
    # Create a container which is passed to other softwares and includes the address ...
    # of all loaded files. They may update it and pass to the next software:
    allLoadedAddress={'patternDataAddress':patternDataAddress}
    """
    Loading the "Pattern" list containing generated traffic patterns as follows:
    - Shape of "Pattern": (500, 4, 180000) --> (# of patterns, 4 types of patterns, length of each pattern)
    Types of patterns are [detected, attempted,congested,freeP] and explained below:
    - Pattern[:,0,:]: Detected packets by the base station (successfully delivered);
    - Pattern[:,1,:]: Total attempts (transmissions) done by all devices to access the network;
    - Pattern[:,2,:]: Count of congested preambles out of avilable preambles (usually: out of 53);
    - Pattern[:,3,:]: Count of not used preambles at each time slot (usually: out of 53).
    NOTE: We use only "detected" and "congested" patterns in this work and the ...\
          rest of them can be used for presentation purposes.
    """
    Pattern=dataLoad.get('Pattern') # Loading the patterns
    """
    Extracting the detected packets and congested preambles pattern:
    - "Pattern" contains 500 sequences which we divide them into three sets:
        - Training set: Strems are randomely chosen from first '400-validationDataSize' samples.
        - Validation set: Fixed streams located between '400-validationDataSize' and '400' positions.
        - Testing set: Strems are randomely chosen from last '100' samples.
    """
    trainingSet=np.random.choice(range(len(Pattern)-100-validationDataSize),\
                                 trainingDataSize, replace=False) # Random choose
    detectedTraffic=np.zeros([trainingDataSize,np.shape(Pattern)[2]]) 
    congestion=np.zeros_like(detectedTraffic)
    for i,loc in enumerate(trainingSet):
        detectedTraffic[i,:]=Pattern[loc][0] # Detected traffic is in the first position
        congestion[i,:]=Pattern[loc][2] # Congestion is in the third position
    validationSet=[i for i in range(len(Pattern)-100-validationDataSize,len(Pattern)-100)]
    valLoss=np.zeros([np.size(TpVec),repeating*int((trainingDataSize-1)/validationFreq)]) # Validation loss container
    valDetectedTraffic=np.zeros([validationDataSize,np.shape(Pattern)[2]])
    valCongestion=np.zeros_like(valDetectedTraffic)
    for i,loc in enumerate(validationSet):
        valDetectedTraffic[i,:]=Pattern[loc][0] # Detected traffic is in the first position
        valCongestion[i,:]=Pattern[loc][2] # Congestion is in the third position
    # Generating the training tensors (input and target) from the selected patterns:
    [batchedInTraffic,batchedTargetTraffic]=list2Tensor(detectedTraffic, batchSize)
    [batchedInCongestion,batchedTragetCongestion]=list2Tensor(congestion, batchSize)
    # The ML network gets traffic and congestion values as input features. The \
    # input and output shape will be [trainingDataSize, batches, batchSize, 2]:
    inputs=torch.cat((batchedInTraffic,batchedInCongestion),3) # Appending traffic and congestion tensors.
    targets=torch.cat((batchedTargetTraffic,batchedTragetCongestion),3) # Appending target tensors.
    # Creating the ML network:
    model = rnnNetwork(inputs[0].size(2),batchSize,rnnLayers,rnnHiddenSize, \
                        dropoutVal,nType=nType,track_states = False,GPU_flag=True)
    model.double()
    # Moving the data on the desired processign device:
    if model.GPU_flag:
        model.cuda()
        device=torch.device('cuda') # Used to move tensors to the desired device (GPU).
    else:
        device=torch.device('cpu') # Used to move tensors to the desired device (CPU).
    model.train() # Training mode activation.
    model.init_hidden(batchSize) # Initializing the network states.
    criterion = nn.MSELoss() # Using MSE metric to calculate the error.
    optimizer = optim.Adam(model.parameters(),lr = 0.0001) # model.parameters() returns weights to be optimized.
    trainingLoss=np.zeros(epochs)  # Training error history
    valCounter=0    # Validation counter
    # Due to memory limits, we need divide the sequence into shorter sequences.\
    # This value may change from a system to the other. For our system, each \
    # sub sequence can include up to 20,000 samples.
    """If you face memory shortage issue, you can reduce the sub-sequence size."""
    subSeqBatches=int(20000/batchSize) # Size of each sub-sequence in batches.
    subSeqLength=int(inputs.size(1)/subSeqBatches) # Total number of sub sequences
    residual=int(batchedInTraffic.size(1)%subSeqBatches) # The last remaining batches
    starttime=datetime.now() # Record the simulation time.
    print('Start time: ',starttime)
    #%% Training the ML network:
    for e in range(repeating):
        for i,(inputSeq,targetSeq) in enumerate(tqdm(zip(inputs,targets),total=trainingDataSize,\
                                             desc='Training RNN network over the training data')):
            model.train()   # Training mode activation.
            model.init_hidden(batchSize)   # Initializing the ML network states only one time for each sequence
            # Dividing each sequence into sub-sequnces. As the last sub-sequence \
            # may have different length, we are using 'list' instead of 'tensor.view()'.
            inSubSeqList=[inputSeq[subSeqBatches*k:subSeqBatches*(k+1)] for \
                           k in range(subSeqLength)] # Dividing the input seq.
            tarSubSeqList=[targetSeq[subSeqBatches*k:subSeqBatches*(k+1)] for \
                           k in range(subSeqLength)] # Dividing the target seq.
            if(residual): # Adding the residual sub-sequences:
                inSubSeqList.append(inputSeq[-residual:])
                tarSubSeqList.append(targetSeq[-residual:])
            subLoss=0 # To add up the loss of each part
            # Training over a single sequence at each time:
            for k,(inSubSeq,tarSubSeq) in enumerate(zip(inSubSeqList,tarSubSeqList)):
                optimizer.zero_grad() # Initializing the gradients 
                out = model(inSubSeq.to(device)) # Get the output
                loss = criterion(out,tarSubSeq.to(device)) # calculate the loss
                subLoss+=loss.item() # Add up the loss of each sub sequence.
                loss.backward()  # Backward propagating the loss
                optimizer.step() # Optimizing the net. parameters
            trainingLoss[e*trainingDataSize+i]=subLoss # Record the loss of each sequence             
            # Evaluating the performance of prediction over the validation data\
            # every validationFreq iterations. Evaluations done for online prediction\
            # mode for each specific 'Tp' in 'TpVec':
            if(i%validationFreq==0 and i!=0):
                with torch.no_grad(): # No gradient calculation --> faster + less memory.
                    for iTp,Tp in enumerate(TpVec):
                        batchesAhead=int(Tp/frameSize/batchSize) # Predicted data length in batches at each step.
                        valSubLoss=0 # Initializing the validation loss for each sequence.
                        for (valTraff, valCong) in zip(valDetectedTraffic,valCongestion):
                            # Reshaping a small portion of validation data into \
                            # batched format for a quick response:
                            [batchedValTraff,BatchedValTarget]=preprocess(valTraff[0:40000],batchSize)
                            [BatchedValCong,BatchedValCongTar]=preprocess(valCong[0:40000],batchSize)
                            valInput=torch.cat((batchedValTraff,BatchedValCong),2)
                            valTarget=torch.cat((BatchedValTarget,BatchedValCongTar),2)
                            valSubLoss += validation(model, batchSize, valInput.to(device),\
                                          valTarget.to(device), seedLength, batchesAhead, \
                                          stepLength, criterion)
                        valLoss[iTp,valCounter]=valSubLoss/validationDataSize  # Validation error of first set 
                    valCounter+=1
    #%% Storing the results:
    print('\nTotal elapsed time after training and validation: ', datetime.now()-starttime)
    Description='This is a result of training a model to predict the network traffic'+\
        f'with following characteristics:\n\t- One {rnnLayers}-layer {nType} with '+\
        f'hidden size of {rnnHiddenSize};\n\t- One DenseNet with 2-layer FFNN;\n\t'+\
        f'- Batch size: {batchSize};\n\t- Training sequences: {trainingDataSize};\n\t'+\
        f'- Total training iterations: {epochs};\n\t'+\
        f'- Sub-sequence size: {subSeqBatches} in batches;\n\t'+\
        f'- Validation sequences: {validationDataSize};\n\t'+\
        f'- Tstep for valdiation: {Tstep} seconds;\n\t- Tseed for valdiation: {Tseed} seconds;\n\t'+\
        f'- Tp Vector for valdiation: {TpVec} in seconds.'
    data={'model':model, 'batchSize':batchSize, 'epochs':epochs,'losshist':trainingLoss,\
          'Description':Description, 'trainingDataSize':trainingDataSize,\
          'repeating':repeating, 'valLoss':valLoss, 'TpVec':TpVec, 'Tseed':Tseed,\
          'Tstep':Tstep, 'allLoadedAddress':allLoadedAddress} # data to be stored.
    # Storing the data on disk:
    dataManagement(data=data, save=True, fileFormat='.pt', fileName='B_Trained'+nType,\
                   version='',path='B_trainedRNN')
    print('End time: ',datetime.now())
    print('Total elapsed time: ',datetime.now()-starttime)
    print(Description)
